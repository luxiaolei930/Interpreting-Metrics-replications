{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.nist_score import sentence_nist \n",
    "from nltk.translate.meteor_score import meteor_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_chinese_char(uchar):\n",
    "    \"\"\"\n",
    "    判断是否中文字符\n",
    "    :param uchar: input char in unicode\n",
    "    :return: whether the input char is a Chinese character.\n",
    "    \"\"\"\n",
    "    _UCODE_RANGES = [\n",
    "        (u'\\u3400', u'\\u4db5'),  # CJK Unified Ideographs Extension A, release 3.0\n",
    "        (u'\\u4e00', u'\\u9fa5'),  # CJK Unified Ideographs, release 1.1\n",
    "        (u'\\u9fa6', u'\\u9fbb'),  # CJK Unified Ideographs, release 4.1\n",
    "        (u'\\uf900', u'\\ufa2d'),  # CJK Compatibility Ideographs, release 1.1\n",
    "        (u'\\ufa30', u'\\ufa6a'),  # CJK Compatibility Ideographs, release 3.2\n",
    "        (u'\\ufa70', u'\\ufad9'),  # CJK Compatibility Ideographs, release 4.1\n",
    "        (u'\\u20000', u'\\u2a6d6'),  # (UTF16) CJK Unified Ideographs Extension B, release 3.1\n",
    "        (u'\\u2f800', u'\\u2fa1d'),  # (UTF16) CJK Compatibility Supplement, release 3.1\n",
    "        (u'\\uff00', u'\\uffef'),  # Full width ASCII, full width of English punctuation,\n",
    "                                 # half width Katakana, half wide half width kana, Korean alphabet\n",
    "        (u'\\u2e80', u'\\u2eff'),  # CJK Radicals Supplement\n",
    "        (u'\\u3000', u'\\u303f'),  # CJK punctuation mark\n",
    "        (u'\\u31c0', u'\\u31ef'),  # CJK stroke\n",
    "        (u'\\u2f00', u'\\u2fdf'),  # Kangxi Radicals\n",
    "        (u'\\u2ff0', u'\\u2fff'),  # Chinese character structure\n",
    "        (u'\\u3100', u'\\u312f'),  # Phonetic symbols\n",
    "        (u'\\u31a0', u'\\u31bf'),  # Phonetic symbols (Taiwanese and Hakka expansion)\n",
    "        (u'\\ufe10', u'\\ufe1f'),\n",
    "        (u'\\ufe30', u'\\ufe4f'),\n",
    "        (u'\\u2600', u'\\u26ff'),\n",
    "        (u'\\u2700', u'\\u27bf'),\n",
    "        (u'\\u3200', u'\\u32ff'),\n",
    "        (u'\\u3300', u'\\u33ff'),\n",
    "    ]\n",
    "    for start, end in _UCODE_RANGES:\n",
    "        if start <= uchar <= end:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def tokenize_with_Chinese(line):\n",
    "    #  分词，中英文混合\n",
    "    # 分词方法参考: SacreBleu.Metrics.tokenizer.tokenizer_zh\n",
    "    _re = [\n",
    "                # language-dependent part (assuming Western languages)\n",
    "                (re.compile(r'([\\{-\\~\\[-\\` -\\&\\(-\\+\\:-\\@\\/])'), r' \\1 '),\n",
    "                # tokenize period and comma unless preceded by a digit\n",
    "                (re.compile(r'([^0-9])([\\.,])'), r'\\1 \\2 '),\n",
    "                # tokenize period and comma unless followed by a digit\n",
    "                (re.compile(r'([\\.,])([^0-9])'), r' \\1 \\2'),\n",
    "                # tokenize dash when preceded by a digit\n",
    "                (re.compile(r'([0-9])(-)'), r'\\1 \\2 '),\n",
    "            ]\n",
    "    line = line.strip()\n",
    "    line_in_chars = \"\"\n",
    "    for char in line:\n",
    "        if _is_chinese_char(char):\n",
    "            line_in_chars += \" \"\n",
    "            line_in_chars += char\n",
    "            line_in_chars += \" \"\n",
    "        else:\n",
    "            line_in_chars += char\n",
    "    line = line_in_chars\n",
    "    for (_re, repl) in _re:\n",
    "        line = _re.sub(repl, line)\n",
    "    return line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(file_name):\n",
    "    \"\"\"\n",
    "    功能：\n",
    "        读入文本数据。   \n",
    "    参数:\n",
    "        file_name(str) - 文件名     \n",
    "    返回:\n",
    "        读入文件中的文本(str)\n",
    "    \"\"\"\n",
    "    with open(file_name,\"r\",encoding = \"utf8\") as f:\n",
    "        text = f.readlines()\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_avg(l):\n",
    "    \"\"\"\n",
    "    功能：\n",
    "        计算平均数\n",
    "    参数：\n",
    "         l-列表list\n",
    "    返回：\n",
    "        平均数\n",
    "    \"\"\"\n",
    "    return sum(l)/len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(sent):\n",
    "    \"\"\"\n",
    "    功能：\n",
    "        单个句子的预处理。   \n",
    "    参数:\n",
    "        sent(str) - 要处理的句子文本     \n",
    "    返回:\n",
    "        处理后的句子(str)\n",
    "    \"\"\"\n",
    "    sent = sent.lower()\n",
    "    punctuation = r\"\"\"!\"#$%&'()*+,./:;<=>?@[\\]^_`{|}~“”？，！【】（）、。：；’‘……￥·\"\"\"\n",
    "    dicts={i:'' for i in punctuation}\n",
    "    punc_table=str.maketrans(dicts)\n",
    "    new_sent = sent.translate(punc_table)\n",
    "    return new_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_bleu(refs, candi):\n",
    "    \"\"\"\n",
    "    功能：\n",
    "        计算BLEU值。   \n",
    "    参数:\n",
    "        refs(List(str)) - 多个参考译文文本 [ref1, ref2, ref3]\n",
    "        candi(str) - 候选译文文本 \"candidate\"\n",
    "    返回:\n",
    "        候选译文的BLEU值(float)\n",
    "    \"\"\"\n",
    "    refs_token = []\n",
    "    for ref in refs:\n",
    "        ref_token = chinese_tokenized(ref)\n",
    "        refs_token.append(ref_token)\n",
    "    candi_token = chinese_tokenized(candi)\n",
    "    score = sentence_bleu(refs_token, candi_token, weights=[0.25,0.25,0.25,0.25])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num_marks(sentence):\n",
    "    \"\"\"\n",
    "    功能：\n",
    "        去除句首定位符的数字。   \n",
    "    参数:\n",
    "        sentence(str) - 待处理的句子\n",
    "    返回:\n",
    "        处理后的句子(str)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if sentence[0].isdigit() and len(sentence)<=2: \n",
    "            return \"\" \n",
    "        # 以上是为了处理文件中存在文中的某一行数据为空的情况\n",
    "        else:\n",
    "            if \".\" not in sentence[:3]: return sentence[2:] \n",
    "            return sentence[sentence.index(\".\")+1:]\n",
    "    except:\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cn_meteor_score(ref_sents, candi_sent):\n",
    "    ref_sents = [\" \".join(chinese_tokenized(ref_sent)) for ref_sent in ref_sents]\n",
    "    candi_sent = \" \".join(chinese_tokenized(candi_sent))\n",
    "    return meteor_score(ref_sents, candi_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_dir(path):\n",
    "    dbtype_list = os.listdir(path)\n",
    "    for dbtype in dbtype_list:\n",
    "        if os.path.isfile(os.path.join(path,dbtype)):\n",
    "            dbtype_list.remove(dbtype)\n",
    "    return dbtype_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(txt_path):\n",
    "    for _,_, file in os.walk(txt_path):\n",
    "        txt_files = file\n",
    "    print(txt_path)\n",
    "    txt_files = [txt_path+t for t in txt_files if t.endswith(\".txt\") and \"没转写完\" not in t]\n",
    "    print(txt_files)\n",
    "    return txt_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_calc(candi_files, ref_files, task, dataset):\n",
    "    # Step1. 读入文本数据\n",
    "\n",
    "    # 存放当前任务的参考译文\n",
    "    references = []\n",
    "#     import pdb;pdb.set_trace()\n",
    "    for f in ref_files:\n",
    "        sentences = read(f)\n",
    "        #     去除句首定位符的数字\n",
    "        sentences = [remove_num_marks(sent) for sent in sentences if sent[0].isdigit()]\n",
    "        sentences = [process_sentence(sent.strip()) for sent in sentences] #strip()去掉收尾的特殊字符，sent[0].isdigit()获取以数字开头的句子, process_sentence预处理每个句子\n",
    "        assert len(sentences)== sentence_num # 断言每个文本的个数为sentence_num\n",
    "    \n",
    "        references.append(sentences)\n",
    "    sentence_num = len(references[0]) # 每个任务中句子的个数\n",
    "\n",
    "    # 存放当前任务的参考译文\n",
    "    candidates = []\n",
    "    for f in candi_files:\n",
    "        sentences = read(f)\n",
    "        # 去除句首定位符的数字\n",
    "        sentences = [remove_num_marks(sent) for sent in sentences if sent[0].isdigit()]\n",
    "        sentences = [process_sentence(sent.strip()) for sent in sentences] #strip()去掉收尾的特殊字符，sent[0].isdigit()获取以数字开头的句子, process_sentence预处理每个句子\n",
    "        print(f\"参考译文句子个数:{sentence_num},候选译文句子个数：{len(sentences)},文件名：{f}\")\n",
    "        assert len(sentences)== sentence_num # 断言每个文本的个数为sentence_num\n",
    "        candidates.append(sentences)\n",
    "    # 计算每个句子的bleu、meteor、nist值\n",
    "    # 遍历所有候选译文\n",
    "    doc_bleu = {}\n",
    "    doc_meteor = {}\n",
    "    doc_nist = {}\n",
    "    for i in range(len(candidates)):\n",
    "        candi_doc = candidates[i]\n",
    "        # 遍历每个候选译文的所有句子\n",
    "        sent_bleu_list = []\n",
    "        sent_meteor_list = []\n",
    "        sent_nist_list = []\n",
    "        for idx in range(len(candi_doc)):\n",
    "            candi_sent = candi_doc[idx]\n",
    "            # 获取当前句子的参考译文\n",
    "            ref_sents = [ref[idx] for ref in references] \n",
    "    #         获取当前句子的bleu、meteor值\n",
    "            score_bleu = cal_bleu(ref_sents, candi_sent)\n",
    "            score_meteor = cn_meteor_score(ref_sents, candi_sent)\n",
    "    #         由于nist内部涉及除法，句子为空会报错。因此，当发生这种情况下，将nist的分数设为0\n",
    "            try:\n",
    "                score_nist = sentence_nist([chinese_tokenized(sent) for sent in ref_sents], chinese_tokenized(candi_sent))\n",
    "            except ZeroDivisionError:\n",
    "                score_nist = 0\n",
    "            sent_bleu_list.append(score_bleu)\n",
    "            sent_meteor_list.append(score_meteor)\n",
    "            sent_nist_list.append(score_nist)\n",
    "    #     将所有句子的分数值求平均作为该候选译文的最终值\n",
    "        doc_bleu[candi_files[i]] = cal_avg(sent_bleu_list)\n",
    "        doc_meteor[candi_files[i]] = cal_avg(sent_meteor_list)\n",
    "        doc_nist[candi_files[i]] = cal_avg(sent_nist_list)\n",
    "        taskname = f\"{task.strip()}_{dataset}_\"\n",
    "    single_result = {\"ID\":[taskname+c.split(\"/\")[-1].replace(\".txt\",\"\") for c in candi_files],taskname+\"BLEU\": doc_bleu.values(), taskname+\"NIST\": doc_nist.values(), taskname+\"METEOR\": doc_meteor.values()}\n",
    "    return pd.DataFrame.from_dict(single_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tasks = [\"1_SI\", \"2_ST\", \"3_ST\", \"4_ST\", \"5_CI\", \"6_CI\", \"7_CI\", \"8_CI\", \"9_CI\"]\n",
    "datasets = []\n",
    "for task in tasks:\n",
    "    task_result_list = []\n",
    "    print(\"#\"*10, task, \"#\"*10)\n",
    "    datasets = get_all_dir(f\"../{task}\")\n",
    "    datasets = [dataset for dataset in datasets if \"EC\" in dataset]\n",
    "    print(datasets)\n",
    "    if len(datasets)==0: continue\n",
    "    for dataset in datasets:\n",
    "        print(\"#\"*5, dataset, \"#\"*5)\n",
    "        candi_path = f\"../{task}/{dataset}/candidates/\"\n",
    "        ref_path = f\"../{task}/{dataset}/references/\"\n",
    "        source_path = f\"../{task}/{dataset}/source/\"\n",
    "        #print(candidates_path, references_path, source_path)\n",
    "        candi_files = read_txt(candi_path)\n",
    "        ref_files = read_txt(ref_path)\n",
    "        task_result = run_calc(candi_files, ref_files, task, dataset)\n",
    "        task_result_list.append(task_result)\n",
    "    task_df = pd.concat(task_result_list,axis=1)\n",
    "    task_df.to_excel(f\"./result/sep/{task}.EC.BLEU.NIST.METEOR.xls\",index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
